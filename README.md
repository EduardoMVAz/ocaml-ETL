# OCaml ETL Project - Eduardo Mendes Vaz

## Summary

The goal of this project is to develop an ETL program that summarizes the data contained on the files under ```/etl/data```. 

The summarization made by the program should group the **orders** (```/etl/data/order.csv```) and their **order items** (```/etl/data/order_item.csv```) into the following example table:

| order_id | total_amount | total_taxes |
|-----------|-------------|-------------|
| 1         | 1345.88     | 20.34       |
| 5         | 34.54       | 2.35        |
| 14        | 334.44      | 30.4        |

The summarization should also receive arguments in which to filter. These arguments relate to the **status** and **origin** fields from the **order** table.

In addition to the main features, 7 custom features were proposed:

- The program should be able to read data from an internet file (exposed via http protocol);
- The program should be able to save data in and SQLite database;
- The program should be able to join the **order** and **order_item** tables before transforming the data;
- The program should be managed using [```dune```](https://dune.readthedocs.io/en/stable/);
- The program's utility functions should be documentated using the **docstring** format;
- The program should be able to create an additional output that groups the average revenue and tax for each month (and year);
- The project should include tests for all pure functions.

The following steps were followed during the development of this project:

- [Initialization](#initialization)
- [Basic Features Implementation](#basic-features-implementation)
- [Custom Features Implementation](#custom-features-implementation)
- [Testing](#testing)

The steps in which Generative AI (such as ChatGPT and Gemini) were have the details of the usage described at the end of their section.

## Initialization 

The initialization step of the development included:

- Creating the dune structure
- Creating the main file for the project
- Creating auxiliary functions to read data (CSV functions)

In this first step, all of the file structure of the project was organized, and the functions on ```/etl/lib/reader.ml``` were developed with use of Generative AI. The functions were created to read csv files into a **list of lists of strings** representing the lines of the csv, with the values already separated.

The csv reading was already done by reading an URL exposed via the http protocol, which satisfies the first custom feature (more on the custom features on the last step).

### * AI Usage

During the development of this section, Generative AI was used for the following purposes:

- Understanding the structure generated by the dune project manager;
- Creating the functions used to read data from csv files with ocaml.

## Basic Features Implementation

In the Basic Features step, all of the logic to create the base output desired was implemented.


The first part of this step was to create **records** to represent the csv tables **order** and **order_items**. Those, alongside with a record for the **joined** table `order_with_items` were created in the ```/etl/lib/records.ml``` file. The joined table was created in order to satisfy another of the custom features (more on the custom features on the last step).


The second part of this step was to create **mappers**: functions that took the **list of lists of strings** representing the lines of the csv and transformed these lines into records of the corresponding types **order** and **order_item** types. These mapper functions were created in the ```/etl/lib/mapper.ml``` file.


A third mapper was created to join the tables, **inner_join_mapper**. This mapper receives the list of order records and the list of order_item records, iterates through the order_item list (given that the order_items represent a larger volume than the orders, due to the n-1 relationship) and matches each order_item with and order based on its order_id, using the List.find_opt function through the order list. Then, it creates an entry for each order_item together with it's order information.


With the orders and their items joined, the next part of this step was to allow the user to filter by **status** and **origin**. This is done by using command line arguments, and was developed with the help of Generative AI. This implementation allows the user to run the main file in the following ways:

        dune exec bin/main.exe -> Creates the output without filtering

        dune exec bin/main.exe -- --s <STATUS> -> Creates the output while filtering orders by the provided status

        dune exec bin/main.exe -- --o <ORIGIN> -> Creates the output while filtering orders by the provided origin

        dune exec bin/main.exe -- --s <STATUS> --o <ORIGIN> -> Creates the output while filtering orders by the provided status and origin

Then, a filtering function **order_filter** was developed in the ```/etl/lib/transformer.ml``` file, which uses List.filter to filter the received order_with_item list created by the join, removing the items that do not fit the desired status or origin.


The next step was to reduce the filtered orders_with_items and create the final output. For that, a reducing function was developed in the ```/etl/lib/transformer.ml``` file, **order_summarize**. The reducing function was developed with help of Generative AI, mainly the idea of using the List.partition function. This reducing function accumulates the orders into a list by reducing them in the following step-by-step:
- First, List.partition is used to separate the entries in the accumulated list so far into two lists: Every item that has the same id as the order_with_item currently being processed, and every other item;
- Then the list that has items with the same id is analyzed. Given that the goal is to reduce entries into only one entry, an error is raised if this list contains more then one item. If this list is empty, a new entry is created for the order_id of the order_with_item currently being processed, and the total amount and total taxes are calculated for it. If the list already contains one item, then an entry has already been created, and the values are updated, substituting the entry by merging the new entry with updated values with the other items separated with List.partition.


The reduced result is, then, an entry for every order_id and the sum of amount and taxes.


Finaly, this reduced result is parsed into the format of a csv, including the headers of the columns, by a function **parse_to_csv**, developed in ```/etl/lib/helper.ml```. The summarized data for the orders is then saved into an output file using the **write_order_total_csv** function, developed in ```/etl/lib/reader.ml```, finishing the Basic Features.

### * AI Usage

During the development of this section, Generative AI was used for the following purposes:

- Learning about the ocaml language (for example, documentation of functions and libs). This was the main use, as documentation for ocaml libs isn't intuitive and many examples of code on the internet (on sites such as stack overflow) have deprecated implementation;
- Creating the code which reads the command line arguments;
- Helping to develop the more complex functions, such as the reducing function, which at first was developed using List.find_opt, but with the help of AI, had the function List.partition used instead, which removed the need to remove the old entry for an order_id.
- Debugging and finding minor syntax errors.


## Custom Features Implementation

During the development of the Basic Features, 3 Custom Features were satisfied:

- The program should be managed using [```dune```](https://dune.readthedocs.io/en/stable/) -> The program was managed with dune since it's start.
- The program should be able to read data from an internet file (exposed via http protocol) -> The program read data from an internet file since it's start, having the csv's read from the project's own github repository.
- The program should be able to join the **order** and **order_item** tables before transforming the data -> The program used the joined table since the start, which was almost the same as filtering the order table before joining with the order_item table.

## Testing
